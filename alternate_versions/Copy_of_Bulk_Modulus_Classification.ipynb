{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Bulk Modulus Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dX8FtlpGJRE6"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb4espuLKJiA"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Hub Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jM3hCI1UUzar"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_NEJlxKKjyI"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/classify_text_with_bert\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/classify_text_with_bert.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://tfhub.dev/google/collections/bert/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub model</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ6SNYq_tVVC"
      },
      "source": [
        "# Classify Bulk Modulus Sentences\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCjmX4zTCkRK"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-YbjCkzw0yU"
      },
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q -U tensorflow-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w_XlxN1IsRJ"
      },
      "source": [
        "You will use the AdamW optimizer from [tensorflow/models](https://github.com/tensorflow/models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-P1ZOA0FkVJ"
      },
      "source": [
        "!pip install -q tf-models-official"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XgTpm9ZxoN9"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6MugfEgDRpY"
      },
      "source": [
        "## Sentiment analysis\n",
        "\n",
        "This notebook trains a sentiment analysis model to classify movie reviews as *positive* or *negative*, based on the text of the review.\n",
        "\n",
        "You'll use the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnvd4mrtPHHV"
      },
      "source": [
        "### Download the dataset\n",
        "\n",
        "Let's download and extract the dataset, then explore the directory structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oub5_iTSQ7SH",
        "outputId": "07ccb898-2ae3-45e6-e249-220e09fda022"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "m69siUoQRXtN",
        "outputId": "fde6e524-4506-429d-ebfa-4167ce8f3e72"
      },
      "source": [
        "os.chdir('gdrive/MyDrive/NLP')\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/MyDrive/NLP'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset(train_path, test_path, train_folder, test_folder):\n",
        "  if train_path != None:\n",
        "    train_csv = pd.read_csv(train_path)\n",
        "    entail_train = train_csv[train_csv[\"true_label\"] == \"entailment\"]\n",
        "    contra_train = train_csv[train_csv[\"true_label\"] == \"contradiction\"]\n",
        "\n",
        "  if test_path != None:\n",
        "    test_csv = pd.read_csv(test_path)\n",
        "    entail_test = test_csv[test_csv[\"true_label\"] == \"entailment\"]\n",
        "    contra_test = test_csv[test_csv[\"true_label\"] == \"contradiction\"]\n",
        "\n",
        "  #process train dataset\n",
        "  if train_folder != None:\n",
        "    #entailment\n",
        "    ent_path = train_folder+\"/entailment\"\n",
        "    if not os.path.exists(ent_path):  \n",
        "      os.makedirs(ent_path) \n",
        "      for i in range(len(entail_train.index)):\n",
        "        with open( f\"{ent_path}/{i}.txt\", \"w\") as f1:\n",
        "          f1.write(entail_train.at[entail_train.index[i],\"sentence\"])\n",
        "\n",
        "    #contradiction\n",
        "    cont_path = train_folder+\"/contradiction\"\n",
        "    if not os.path.exists(cont_path): \n",
        "      os.makedirs(cont_path)\n",
        "      for i in range(len(contra_train.index)):\n",
        "        with open( f\"{cont_path}/{i}.txt\", \"w\") as f2:\n",
        "          f2.write(contra_train.at[contra_train.index[i],\"sentence\"])\n",
        "\n",
        "  #process test dataset\n",
        "  if test_folder != None:\n",
        "    #entailment\n",
        "    ent_path = test_folder+\"/entailment\"\n",
        "    if not os.path.exists(ent_path):  \n",
        "      os.makedirs(ent_path) \n",
        "      for i in range(len(entail_test.index)):\n",
        "        with open( f\"{ent_path}/{i}.txt\", \"w\") as f3:\n",
        "          f3.write(entail_test.at[entail_test.index[i],\"sentence\"])\n",
        "        \n",
        "    #contradiction\n",
        "    cont_path = test_folder+\"/contradiction\"\n",
        "    if not os.path.exists(cont_path):\n",
        "      os.makedirs(cont_path)\n",
        "      for i in range(len(contra_test.index)):\n",
        "        with open( f\"{cont_path}/{i}.txt\", \"w\") as f4:\n",
        "          f4.write(contra_test.at[contra_test.index[i],\"sentence\"])\n"
      ],
      "metadata": {
        "id": "hBCdgbWl1CXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset(\"dataset/syn_aug_pos_full.csv\", None, \"dataset/train_syn\", None)\n",
        "dataset(\"dataset/BT_aug_pos_full.csv\", None, \"dataset/train_BT\", None)\n",
        "dataset(\"dataset/bert_aug_pos_full.csv\", None, \"dataset/train_bert\", None)"
      ],
      "metadata": {
        "id": "rf3mhd3mAlf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iW529XtovDL"
      },
      "source": [
        "test_csv = pd.read_csv(\"dataset/test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eZ5SDekWRTUi",
        "outputId": "6e5c01be-8d6e-44e2-93d4-0cdbd748d773"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/MyDrive/NLP'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN9lWCYfPo7b"
      },
      "source": [
        "Next, you will use the `text_dataset_from_directory` utility to create a labeled `tf.data.Dataset`.\n",
        "\n",
        "The IMDB dataset has already been divided into train and test, but it lacks a validation set. Let's create a validation set using an 80:20 split of the training data by using the `validation_split` argument below.\n",
        "\n",
        "Note:  When using the `validation_split` and `subset` arguments, make sure to either specify a random seed, or to pass `shuffle=False`, so that the validation and training splits have no overlap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IwI_2bcIeX8",
        "outputId": "17c44e13-0bba-417b-ad89-56eda99ed4ee"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "train_folder = \"dataset/train_bert\"\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    train_folder,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "\n",
        "class_names = raw_train_ds.class_names\n",
        "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    train_folder,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'dataset/test',\n",
        "    batch_size=batch_size)\n",
        "\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 930 files belonging to 2 classes.\n",
            "Using 744 files for training.\n",
            "Found 930 files belonging to 2 classes.\n",
            "Using 186 files for validation.\n",
            "Found 2677 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX8FtlpGJRE6"
      },
      "source": [
        "## Loading models from TensorFlow Hub\n",
        "\n",
        "Here you can choose which BERT model you will load from TensorFlow Hub and fine-tune. There are multiple BERT models available.\n",
        "\n",
        "  - [BERT-Base](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3), [Uncased](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3) and [seven more models](https://tfhub.dev/google/collections/bert/1) with trained weights released by the original BERT authors.\n",
        "  - [Small BERTs](https://tfhub.dev/google/collections/bert/1) have the same general architecture but fewer and/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.\n",
        "  - [ALBERT](https://tfhub.dev/google/collections/albert/1): four different sizes of \"A Lite BERT\" that reduces model size (but not computation time) by sharing parameters between layers.\n",
        "  - [BERT Experts](https://tfhub.dev/google/collections/experts/bert/1): eight models that all have the BERT-base architecture but offer a choice between different pre-training domains, to align more closely with the target task.\n",
        "  - [Electra](https://tfhub.dev/google/collections/electra/1) has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).\n",
        "  - BERT with Talking-Heads Attention and Gated GELU [[base](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1), [large](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1)] has two improvements to the core of the Transformer architecture.\n",
        "\n",
        "The model documentation on TensorFlow Hub has more details and references to the\n",
        "research literature. Follow the links above, or click on the [`tfhub.dev`](http://tfhub.dev) URL\n",
        "printed after the next cell execution.\n",
        "\n",
        "The suggestion is to start with a Small BERT (with fewer parameters) since they are faster to fine-tune. If you like a small model but with higher accuracy, ALBERT might be your next option. If you want even better accuracy, choose\n",
        "one of the classic BERT sizes or their recent refinements like Electra, Talking Heads, or a BERT Expert.\n",
        "\n",
        "Aside from the models available below, there are [multiple versions](https://tfhub.dev/google/collections/transformer_encoders_text/1) of the models that are larger and can yield even better accuracy, but they are too big to be fine-tuned on a single GPU. You will be able to do that on the [Solve GLUE tasks using BERT on a TPU colab](https://www.tensorflow.org/text/tutorials/bert_glue).\n",
        "\n",
        "You'll see in the code below that switching the tfhub.dev URL is enough to try any of these models, because all the differences between them are encapsulated in the SavedModels from TF Hub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8_ctG55-uTX"
      },
      "source": [
        "# #@title Choose a BERT model to fine-tune\n",
        "\n",
        "# bert_model_name = 'small_bert/bert_en_uncased_L-8_H-768_A-12'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "# map_name_to_handle = {\n",
        "#     'bert_en_uncased_L-12_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "#     'bert_en_cased_L-12_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "#     'bert_multi_cased_L-12_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "#     'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "#     'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "#     'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "#     'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "#     'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "#     'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "#     'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "#     'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "#     'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "#     'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "#     'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "#     'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "#     'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "#     'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "#     'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "#     'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "#     'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "#     'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "#     'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "#     'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "#     'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "#     'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "#     'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "#     'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "#     'albert_en_base':\n",
        "#         'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "#     'electra_small':\n",
        "#         'https://tfhub.dev/google/electra_small/2',\n",
        "#     'electra_base':\n",
        "#         'https://tfhub.dev/google/electra_base/2',\n",
        "#     'experts_pubmed':\n",
        "#         'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "#     'experts_wiki_books':\n",
        "#         'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "#     'talking-heads_base':\n",
        "#         'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "# }\n",
        "\n",
        "# map_model_to_preprocess = {\n",
        "#     'bert_en_uncased_L-12_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'bert_en_cased_L-12_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'bert_multi_cased_L-12_H-768_A-12':\n",
        "#         'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "#     'albert_en_base':\n",
        "#         'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "#     'electra_small':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'electra_base':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'experts_pubmed':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'experts_wiki_books':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "#     'talking-heads_base':\n",
        "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "# }\n",
        "\n",
        "# tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "# tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "# print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "# print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHCK3WQppDye"
      },
      "source": [
        "## Load Sci-BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlDikkgjDRN9"
      },
      "source": [
        "# import tarfile\n",
        "# tar = tarfile.open(\"scibert_encoder.tar.gz\")\n",
        "# tar.extractall()\n",
        "# tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-W3qDczJRCo"
      },
      "source": [
        "# tar2 = tarfile.open(\"scibert_preprocess.tar.gz\")\n",
        "# tar2.extractall()\n",
        "# tar2.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKnLPSEmtp9i"
      },
      "source": [
        "## Using the Sci-BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6osbW8zIjEk"
      },
      "source": [
        "tfhub_handle_encoder = tf.saved_model.load(\"encoder_export\")\n",
        "tfhub_handle_preprocess = tf.saved_model.load(\"bert_preprocessing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm61jDrezAll"
      },
      "source": [
        "The BERT models return a map with 3 important keys: `pooled_output`, `sequence_output`, `encoder_outputs`:\n",
        "\n",
        "- `pooled_output` represents each input sequence as a whole. The shape is `[batch_size, H]`. You can think of this as an embedding for the entire movie review.\n",
        "- `sequence_output` represents each input token in the context. The shape is `[batch_size, seq_length, H]`. You can think of this as a contextual embedding for every token in the movie review.\n",
        "- `encoder_outputs` are the intermediate activations of the `L` Transformer blocks. `outputs[\"encoder_outputs\"][i]` is a Tensor of shape `[batch_size, seq_length, 1024]` with the outputs of the i-th Transformer block, for `0 <= i < L`. The last value of the list is equal to `sequence_output`.\n",
        "\n",
        "For the fine-tuning you are going to use the `pooled_output` array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDNKfAXbDnJH"
      },
      "source": [
        "## Define your model\n",
        "\n",
        "You will create a very simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer.\n",
        "\n",
        "Note: for more information about the base model's input and output you can follow the model's URL for documentation. Here specifically, you don't need to worry about it because the preprocessing model will take care of that for you.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aksj743St9ga"
      },
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.05)(net)\n",
        "  # net = tf.keras.layers.Dense(128, activation=None, name='classifier1')(net)# hidden layer 1\n",
        "  # net = tf.keras.layers.Dense(128, activation=None, name='classifier2')(net)# hidden layer 2\n",
        "  # net = tf.keras.layers.Dense(64, activation=None, name='classifier3')(net) # hidden layer 3\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs4yhFraBuGQ"
      },
      "source": [
        "Let's check that the model runs with the output of the preprocessing model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGMF8AZcB2Zy"
      },
      "source": [
        "classifier_model = build_classifier_model()\n",
        "# bert_raw_result = classifier_model(tf.constant(text_test))\n",
        "# print(tf.sigmoid(bert_raw_result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQZgzqhLJiyW"
      },
      "source": [
        "#classifier_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTUzNV2JE2G3"
      },
      "source": [
        "The output is meaningless, of course, because the model has not been trained yet.\n",
        "\n",
        "Let's take a look at the model's structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EmzyHZXKIpm"
      },
      "source": [
        "# tf.keras.utils.plot_model(classifier_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbUWoZMwc302"
      },
      "source": [
        "## Model training\n",
        "\n",
        "You now have all the pieces to train a model, including the preprocessing module, BERT encoder, data, and classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpJ3xcwDT56v"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "Since this is a binary classification problem and the model outputs a probability (a single-unit layer), you'll use `losses.BinaryCrossentropy` loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWPOZE-L3AgE"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77psrpfzbxtp"
      },
      "source": [
        "### Optimizer\n",
        "\n",
        "For fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as [AdamW](https://arxiv.org/abs/1711.05101).\n",
        "\n",
        "For the learning rate (`init_lr`), you will use the same schedule as BERT pre-training: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (`num_warmup_steps`). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9eP2y9dbw32"
      },
      "source": [
        "epochs = 6\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqlarlpC_v0g"
      },
      "source": [
        "### Loading the BERT model and training\n",
        "\n",
        "Using the `classifier_model` you created earlier, you can compile the model with the loss, metric and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7GPDhR98jsD"
      },
      "source": [
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEYSh_b2YZbg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e48212a2-7243-4f60-c970-1bd19968ffb0"
      },
      "source": [
        "  print(f'Training model with {tfhub_handle_encoder}')\n",
        "  history = classifier_model.fit(x=train_ds,\n",
        "                               validation_data=val_ds,\n",
        "                               epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with <tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7f99fb105c90>\n",
            "Epoch 1/6\n",
            "24/24 [==============================] - 34s 644ms/step - loss: 0.5081 - binary_accuracy: 0.7406 - val_loss: 0.2676 - val_binary_accuracy: 0.9086\n",
            "Epoch 2/6\n",
            "24/24 [==============================] - 14s 576ms/step - loss: 0.1911 - binary_accuracy: 0.9247 - val_loss: 0.1251 - val_binary_accuracy: 0.9355\n",
            "Epoch 3/6\n",
            "24/24 [==============================] - 14s 578ms/step - loss: 0.0786 - binary_accuracy: 0.9651 - val_loss: 0.1717 - val_binary_accuracy: 0.9301\n",
            "Epoch 4/6\n",
            "24/24 [==============================] - 14s 576ms/step - loss: 0.0130 - binary_accuracy: 0.9973 - val_loss: 0.1084 - val_binary_accuracy: 0.9624\n",
            "Epoch 5/6\n",
            "24/24 [==============================] - 14s 576ms/step - loss: 0.0033 - binary_accuracy: 1.0000 - val_loss: 0.1278 - val_binary_accuracy: 0.9570\n",
            "Epoch 6/6\n",
            "24/24 [==============================] - 14s 576ms/step - loss: 0.0111 - binary_accuracy: 0.9973 - val_loss: 0.1397 - val_binary_accuracy: 0.9516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpBuV5j2cS_b"
      },
      "source": [
        "Note: training time will vary depending on the complexity of the BERT model you have selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtn7jewb6dg4"
      },
      "source": [
        "## Testing\n",
        "\n",
        "Now you just save your fine-tuned model for later use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41fyBLvkJWn0"
      },
      "source": [
        "#test_csv = pd.read_csv(\"dataset/test.csv\")\n",
        "test_csv = test_csv.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "H0ly73Vg6-zU",
        "outputId": "f8a25fe0-73ed-4789-cb05-0892ddc8a1d3"
      },
      "source": [
        "test_csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>true_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The effect of grain boundaries on the effectiv...</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>At the macro-level the continuum mechanics des...</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In view of the proposed geometrical idealizati...</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Such variation of elastic moduli with a grain ...</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Two single Kelvin moduli h 1 and h 2 are two s...</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2672</th>\n",
              "      <td>The elastic modulus for the bulk glass Zr52.5N...</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2673</th>\n",
              "      <td>After 703 K annealed, the glass phase almost f...</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2674</th>\n",
              "      <td>The bulk glass structure has a lowest elastic ...</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2675</th>\n",
              "      <td>The calculated bulk modulus, using both LDA an...</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2676</th>\n",
              "      <td>The bulk moduli of CsCl- and FeSi-type RuSi we...</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2677 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence     true_label\n",
              "0     The effect of grain boundaries on the effectiv...  contradiction\n",
              "1     At the macro-level the continuum mechanics des...  contradiction\n",
              "2     In view of the proposed geometrical idealizati...  contradiction\n",
              "3     Such variation of elastic moduli with a grain ...  contradiction\n",
              "4     Two single Kelvin moduli h 1 and h 2 are two s...  contradiction\n",
              "...                                                 ...            ...\n",
              "2672  The elastic modulus for the bulk glass Zr52.5N...     entailment\n",
              "2673  After 703 K annealed, the glass phase almost f...     entailment\n",
              "2674  The bulk glass structure has a lowest elastic ...     entailment\n",
              "2675  The calculated bulk modulus, using both LDA an...     entailment\n",
              "2676  The bulk moduli of CsCl- and FeSi-type RuSi we...     entailment\n",
              "\n",
              "[2677 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyTappHTvNCz"
      },
      "source": [
        "Here you can test your model on any sentence you want, just add to the examples variable below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBWzH6exlCPS"
      },
      "source": [
        "#fp_L = []; fn_L = []; tp_L = []; tn_L = []\n",
        "tp = 0; tn = 0; fp = 0; fn = 0\n",
        "test_result = {\"prediction\":[], \"probability\":[]}\n",
        "\n",
        "for i in range(len(test_csv.index)):\n",
        "  sentence = [test_csv.at[test_csv.index[i],\"sentence\"]]\n",
        "  test_label_fl = float(tf.sigmoid(classifier_model(tf.constant(sentence))))\n",
        "  test_label = int(round(test_label_fl))\n",
        "  test_result[\"probability\"].append(test_label_fl)\n",
        "\n",
        "  true_label = test_csv.at[test_csv.index[i],\"true_label\"]\n",
        "\n",
        "  if test_label == 0:\n",
        "    #test_result[\"prediction\"].append(\"contradiction\")\n",
        "\n",
        "    if true_label == \"contradiction\":\n",
        "      tn += 1\n",
        "      #tn_L.append(sentence)\n",
        "    elif true_label == \"entailment\":\n",
        "      fn += 1\n",
        "      #fn_L.append(sentence)\n",
        "\n",
        "  elif test_label == 1:\n",
        "    #test_result[\"prediction\"].append(\"entailment\")\n",
        "\n",
        "    if true_label == \"contradiction\":\n",
        "      fp += 1\n",
        "      #fp_L.append(sentence)\n",
        "    elif true_label == \"entailment\":\n",
        "      tp += 1\n",
        "      #tp_L.append(sentence)\n",
        "\n",
        "  #print(f\"input: {sentence} \\n test_label: {test_label, test_label_fl} \\n true_label: {true_label} \\n\")\n",
        "\n",
        "#test_result_df = pd.DataFrame.from_dict(test_result)\n",
        "#test_result_df.to_csv(\"test_model_result.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx-WSLmkXT5o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "356b0a76-879e-49e1-9af3-a392c639705d"
      },
      "source": [
        "recall = tp/(tp+fn)\n",
        "precision = tp/(tp+fp)\n",
        "f1 = (2*recall*precision) / (recall + precision)\n",
        "\n",
        "print(tp,fn,tn,fp)\n",
        "print(recall,precision,f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82 41 2515 39\n",
            "0.6666666666666666 0.6776859504132231 0.6721311475409837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bert_aug\n",
        "recall_L = [0.7235772357723578,0.6991869918699187,0.6341463414634146,0.6666666666666666,0.6666666666666666]\n",
        "prec_L = [0.6793893129770993,0.7049180327868853,0.6341463414634146,0.6666666666666666,0.6776859504132231]   \n",
        "# countL = [(89 34 2512 42),(86 37 2518 36),(78 45 2509 45),(82 41 2513 41),(82 41 2515 39)]\n"
      ],
      "metadata": {
        "id": "jIlBGQOpvVJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ho0zhXtz71g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c460d2b-e2b8-418a-e9c0-5b8a01bcb46d"
      },
      "source": [
        "avg_recall = sum(recall_L)/len(recall_L)\n",
        "avg_pre = sum(prec_L)/len(prec_L)\n",
        "avg_recall,avg_pre"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6780487804878048, 0.6725612608614577)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BT_aug\n",
        "recall_L = [0.8780487804878049,0.7886178861788617,0.8617886178861789,0.8373983739837398,0.8211382113821138]\n",
        "prec_L = [0.5901639344262295,0.6423841059602649,0.6708860759493671,0.5885714285714285,0.5738636363636364]   \n",
        "# countL = [(108 15 2479 75),(97 26 2500 54),(106 17 2502 52),(103 20 2482 72),(101 22 2479 75)]\n"
      ],
      "metadata": {
        "id": "GSGrvq3YekJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#syn_aug\n",
        "recall_L = [0.7479674796747967,0.6585365853658537,0.7398373983739838,0.6747967479674797,0.7642276422764228]\n",
        "prec_L = [0.6764705882352942,0.7363636363636363,0.7054263565891473,0.7757009345794392,0.6861313868613139]\n",
        "f1_L = [0.7104247104247104,0.6952789699570815,0.7222222222222223,0.7217391304347825,0.7230769230769232]     \n",
        "# countL = [(92 31 2510 44),(81 42 2525 29),(91 32 2516 38),(83 40 2530 24),(94 29 2511 43)]\n"
      ],
      "metadata": {
        "id": "LA4d0z_cUNRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMqx_2Mew9EA"
      },
      "source": [
        "# Sci-BERT avg (smaller testing dataset)\n",
        "recall_L = [0.8292682926829268,0.8536585365853658,0.8536585365853658,0.8130081300813008,0.8780487804878049]\n",
        "prec_L = [0.7083333333333334,0.6818181818181818,0.6441717791411042,0.5988023952095808,0.6242774566473989]\n",
        "f1_L = [0.7640449438202247,0.7581227436823105,0.7342657342657342,0.6896551724137931,0.7297297297297297]     \n",
        "# countL = [(102 21 2512 42),(105 18 2505 49),(105 18 2496 58),(100 23 2487 67),(108 15 2489 65)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_9cyYyJglMf"
      },
      "source": [
        "# Sci-BERT with NN layers avg \n",
        "# recall_L = [0.8292682926829268,\n",
        "#             0.7804878048780488,\n",
        "#             0.8211382113821138,\n",
        "#             0.8536585365853658,\n",
        "#             0.7642276422764228\n",
        "#             ]\n",
        "# prec_L = [0.6181818181818182,\n",
        "#           0.6906474820143885,\n",
        "#           0.6158536585365854,\n",
        "#           0.6521739130434783,\n",
        "#           0.7286821705426356\n",
        "#           ]\n",
        "# f1_L = [0.7083333333333334,\n",
        "#         0.7328244274809161,\n",
        "#         0.7038327526132404,\n",
        "#         0.7394366197183099,\n",
        "#         0.746031746031746\n",
        "#         ]\n",
        "# countL = [(102 21 2491 63),\n",
        "#           (96 27 2511 43),\n",
        "#           (101 22 2491 63),\n",
        "#           (105 18 2498 56),\n",
        "#           (94 29 2519 35)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7MvHiMnY8Un"
      },
      "source": [
        "# Sci-BERT avg (10k testing dataset)\n",
        "# recall_L = [0.8307692307692308,\n",
        "#             0.8,\n",
        "#             0.7538461538461538,\n",
        "#             0.7846153846153846,\n",
        "#             0.7384615384615385]\n",
        "# prec_L = [0.6585365853658537,\n",
        "#           0.5977011494252874,\n",
        "#           0.7205882352941176,\n",
        "#           0.6219512195121951,\n",
        "#           0.7111111111111111]\n",
        "# f1_L = [0.7346938775510204,\n",
        "#         0.6842105263157896,\n",
        "#         0.7368421052631577,\n",
        "#         0.6938775510204082,\n",
        "#         0.7245283018867924]\n",
        "# countL = [(108, 22, 9918, 56),\n",
        "#           (104, 26, 9904, 70),\n",
        "#           (98, 32, 9936, 38),\n",
        "#           (102, 28, 9912, 62),\n",
        "#           (96, 34, 9935, 39)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUgaprdWnr1M"
      },
      "source": [
        "# #experts_pubmed\n",
        "# recall_L = [0.823076923076923\n",
        "#             ]\n",
        "# prec_L = [0.535\n",
        "#          ]\n",
        "# f1_L = [0.6484848484848486]\n",
        "# countL = [(107, 23, 9881, 93)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cOmih754Y_M"
      },
      "source": [
        "If you want to use your model on [TF Serving](https://www.tensorflow.org/tfx/guide/serving), remember that it will call your SavedModel through one of its named signatures. In Python, you can test them as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FdVD3973S-O"
      },
      "source": [
        "# serving_results = reloaded_model \\\n",
        "#             .signatures['serving_default'](tf.constant(examples))\n",
        "\n",
        "# serving_results = tf.sigmoid(serving_results['classifier'])\n",
        "\n",
        "# print_my_examples(examples, serving_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4gN1KwReLPN"
      },
      "source": [
        "## Model Prediction of Testing dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8c93tTKDdPS"
      },
      "source": [
        "pred_df = pd.read_csv(\"test_model_result.csv\")\n",
        "pred_df=pred_df.dropna()\n",
        "pred_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZWhcFx1ELA1"
      },
      "source": [
        "import numpy as np\n",
        "pred_df['2_pred_labels_diff'] = np.where(pred_df['prediction1'] == pred_df['prediction2'], 0, 1)\n",
        "pred_df['combined_pred_label'] = np.where(pred_df['prediction1'] == pred_df['prediction2'], pred_df[\"prediction1\"], (pred_df[\"probability1\"] + pred_df[\"probability2\"])/2)\n",
        "\n",
        "tp = 0; tn = 0; fp = 0; fn = 0\n",
        "\n",
        "#combined_pred_label: turn number into label \n",
        "for i in range(len(pred_df.index)):\n",
        "  idx = pred_df.index[i]\n",
        "  score = pred_df.at[idx,\"combined_pred_label\"]\n",
        "  true_label = pred_df.at[idx,\"true_label\"]\n",
        "\n",
        "  if type(score) == float:\n",
        "    score = int(round(score))\n",
        "    if score == 0:\n",
        "      pred_df.at[idx,\"combined_pred_label\"] = \"contradiction\"\n",
        "      if true_label == \"contradiction\":\n",
        "        tn+=1\n",
        "      else:\n",
        "        fn+=1\n",
        "\n",
        "    elif score == 1:\n",
        "      pred_df.at[idx,\"combined_pred_label\"] = \"entailment\"\n",
        "      if true_label == \"entailment\":\n",
        "        tp+=1\n",
        "      else:\n",
        "        fp+=1\n",
        "\n",
        "  elif type(score) == str:\n",
        "    pred_label = pred_df.at[idx,\"combined_pred_label\"]\n",
        "    if pred_label == \"contradiction\":  \n",
        "      if pred_label == true_label:\n",
        "        tn += 1\n",
        "      else:\n",
        "        fn += 1\n",
        "\n",
        "    elif pred_label == \"entailment\":\n",
        "      if pred_label == true_label:\n",
        "        tp += 1\n",
        "      else:\n",
        "        fp += 1\n",
        "\n",
        "#compare with the true label to count # of wrong predictions\n",
        "pred_df['final_true_pred_diff'] = np.where(pred_df['true_label'] == pred_df['combined_pred_label'], 0, 1)\n",
        "\n",
        "print(tp,fn,tn,fp)\n",
        "\n",
        "recall = tp/(tp+fn)\n",
        "precision = tp/(tp+fp)\n",
        "f1 = (2*recall*precision) / (recall + precision)\n",
        "print(recall,precision,f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Dl3pOM1Hheh"
      },
      "source": [
        "#pred_df.to_csv(\"test_pred_final.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzo6MkddcUrT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}